###This script will initiate the analysis of all the sequences from MC57 and MC58 with humanized mice with C57BL/6 and NOD wild-type mice.

#!/bin/sh -login
#PBS -A ged 
#PBS -l walltime=03:00:00:00,nodes=01:ppn=8,mem=80gb
#PBS -M philliptbrooks@gmail.com

	cd ${PBS_O_WORKDIR} 
	
###make fastq files read only 
	
	chmod u-w ~brooksph/humanized_mouse_experiments_16S/fastq/*

###link data from fastq directory including trainset  

	ln -fs ~brooksph/humanized_mouse_experiments_16S/fastq/*fastq.gz .
	
###load mothur and call it 

	module load mothur/1.36.1
	
	mothur

###make the make file 

	make.file(inputdir=., type=gz)


###make contigs and join the paired ends

	make.contigs(file=current, processors=8)
	
###Screen seqs
###Get rid of sequences that are the wrong length or have ambiguous base pairs.
###We're limiting the length to 275 because we expect our sequences to be about 251 bp
###and this gives some flexibility

	screen.seqs(fasta=current, group=current, maxambig=0, maxlength=275)

###Get only the unique sequences
###This reduces our data set size for further processing
###No sequences are lost during this step. We're just generating a FASTA file of the ###unique
###sequences and keeping track of their numbers in each sample in a different file.
	
	unique.seqs()

###Simplify the names and groups files 	

	count.seqs(name=current, group=current)
	
###Align our sequences to that reference
###This is one of the most computationally intensive steps in this file

	align.seqs(fasta=current, reference=silva.v4.fasta)

###Get rid of sequences that don't align well

	screen.seqs(fasta=current, count=current, start=1968, end=11550, maxhomop=8)

###To reduce dataset size, get rid of columns with no information
###Get rid of columns in the file that only contain gaps '-' or overhang '.'

	filter.seqs(fasta=current, vertical=T, trump=.)

###Now that we've aligned the sequences, we might see more that are identical
###Pare the dataset down to just the unique aligned ones


	unique.seqs(fasta=current, count=current)

###Pre-cluster to reduce dataset size

	pre.cluster(fasta=current, count=current, diffs=2)'

###Check for chimeras and remove them
###We're doing the chimera checking using the abundant reads as the reference
###Alternatively you could use the silva database as a reference

	chimera.uchime(fasta=current, count=current, dereplicate=t)

	remove.seqs(fasta=current, accnos=current)

###Classify the sequences and remove ones that aren't bacterial
###We're using the Silva training sets as references: trainset9_032013

	classify.seqs(fasta=current, count=current, reference=trainset9_032012.pds.fasta, taxonomy=trainset9_032012.pds.tax, cutoff=80)

	remove.lineage(fasta=current, count=current, taxonomy=current, taxon=Chloroplast-Mitochondria-unknown-Archaea-Eukaryota)
	
	###Print summary 
	
	summary.tax(taxonomy=current, count=current)
